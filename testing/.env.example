# DoctorFollow Medical Search Agent - Environment Variables
# Copy this file to .env and fill in your credentials

# ============================================
# AWS Bedrock (for LLM)
# ============================================
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_aws_access_key_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_key_here

# ============================================
# Database Connections (match docker-compose.yml)
# ============================================

# OpenSearch
OPENSEARCH_HOST=localhost
OPENSEARCH_PORT=9200

# PostgreSQL + pgvector
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=doctorfollow
POSTGRES_USER=doctor
POSTGRES_PASSWORD=follow123

# Neo4j
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=doctorfollow123

# ============================================
# Model Configuration
# ============================================

# LLM Model (use cross-region inference profile)
BEDROCK_MODEL_ID=us.meta.llama4-scout-17b-instruct-v1:0

# Embedding Model (multilingual for Turkish â†” English)
# Options:
#   - intfloat/multilingual-e5-large (1024 dim, best quality)
#   - intfloat/multilingual-e5-base (768 dim, faster)
#   - intfloat/multilingual-e5-small (384 dim, fastest)
EMBEDDING_MODEL=intfloat/multilingual-e5-large
EMBEDDING_DIMENSION=1024

# ============================================
# RAG Parameters
# ============================================

# Chunking
CHUNK_SIZE=400
CHUNK_OVERLAP=100

# Retrieval
TOP_K_OPENSEARCH=10
TOP_K_PGVECTOR=10
TOP_K_FINAL=5
RRF_K=60

# LLM Generation
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=512
